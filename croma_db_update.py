from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import ChatOllama
from tqdm import tqdm
from typing import List, Optional
from sklearn.mixture import GaussianMixture as GMM
import chromadb
import umap
import hashlib
import os
import json
import numpy as np
import pandas as pd
import glob

# --- Ayarlar ---
CHROMA_HOST = "localhost"  # Sadece ana bilgisayar adÄ±
CHROMA_PORT = 8000  # Sadece port numarasÄ±
DB_FOLDER = "./database"  # JSON dosyalarÄ±nÄ±n olduÄŸu klasÃ¶r
COLLECTION_NAME = "raptor_knowledge_base"
model_kwargs = {"device": "cuda"}
encode_kwargs = {
    "normalize_embeddings": True
}  # Cosine similarity iÃ§in normalizasyon iyidir
EMBEDDING_MODEL = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-mpnet-base-v2",
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs,
)
MAX_BATCH_SIZE = 5000
CLOUDFLARE_TUNNEL_URL = ".../"
OLLAMA_FAST_MODEL = "llama3.1:8b"
LLM_MODEL = ChatOllama(
    model=OLLAMA_FAST_MODEL, base_url=CLOUDFLARE_TUNNEL_URL, temperature=0
)


class RaptorManager:
    def __init__(self):
        print(f"ğŸ“¡ ChromaDB Server'a baÄŸlanÄ±lÄ±yor (localhost:8000)...")
        self.client = chromadb.HttpClient(host="localhost", port=8000)

        self.vectorstore = Chroma(
            client=self.client,
            collection_name=COLLECTION_NAME,
            embedding_function=EMBEDDING_MODEL,
        )

        # Text Splitter: JSON iÃ§eriÄŸini kÃ¼Ã§Ã¼k parÃ§alara bÃ¶lmek iÃ§in
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500, chunk_overlap=50  # MPNet iÃ§in ideal boyutlar
        )

    def _calculate_file_hash(self, file_path: str) -> str:
        """DosyanÄ±n MD5 hash'ini hesaplar. Ä°Ã§erik deÄŸiÅŸirse bu hash deÄŸiÅŸir."""
        hasher = hashlib.md5()
        with open(file_path, "rb") as f:
            buf = f.read()
            hasher.update(buf)
        return hasher.hexdigest()

    def _load_json_and_split(self, file_path: str) -> List[Document]:
        """JSON dosyasÄ±nÄ± okur ve Document objelerine Ã§evirir."""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)

            # JSON yapÄ±na gÃ¶re burayÄ± dÃ¼zenleyebilirsin.
            # VarsayÄ±m: JSON bir liste veya string iÃ§eriyor.
            # TÃ¼m iÃ§eriÄŸi string'e Ã§evirip split ediyoruz.
            text_content = json.dumps(data, ensure_ascii=False)

            docs = self.text_splitter.create_documents([text_content])
            return docs
        except Exception as e:
            print(f"âŒ Hata: {file_path} okunamadÄ±. Sebep: {e}")
            return []

    def _delete_file_from_db(self, filename: str):
        """Verilen dosya adÄ±na sahip tÃ¼m kayÄ±tlarÄ± DB'den siler."""
        print(f"ğŸ—‘ï¸  Eski kayÄ±tlar siliniyor: {filename}")
        try:
            # ChromaDB'den 'original_source' metadata'sÄ± eÅŸleÅŸenleri sil
            # Not: LangChain Chroma wrapper'Ä±nda doÄŸrudan delete by metadata bazen zordur,
            # bu yÃ¼zden native client kullanÄ±yoruz.
            collection = self.client.get_collection(COLLECTION_NAME)
            collection.delete(where={"original_source": filename})
        except Exception as e:
            print(f"Silme iÅŸlemi uyarÄ±sÄ± (ilk Ã§alÄ±ÅŸtÄ±rma olabilir): {e}")

    def _check_if_exists_and_current(self, filename: str, current_hash: str) -> bool:
        """Dosya DB'de var mÄ± ve Hash'i gÃ¼ncel mi kontrol eder."""
        try:
            collection = self.client.get_collection(COLLECTION_NAME)
            # Sadece 1 tane Ã¶rnek kayÄ±t Ã§ekip hash kontrolÃ¼ yapÄ±yoruz
            results = collection.get(
                where={"original_source": filename}, limit=1, include=["metadatas"]
            )

            if len(results["ids"]) > 0:
                stored_hash = results["metadatas"][0].get("file_hash", "")
                if stored_hash == current_hash:
                    return True  # Dosya var ve gÃ¼ncel
            return False  # Dosya yok veya gÃ¼ncel deÄŸil
        except Exception:
            return False  # Collection yoksa false dÃ¶ner

    def _add_texts_in_batches(self, texts, metadatas, batch_size=5000):
        """Verileri batch'ler halinde ChromaDB'ye ekler."""
        total_docs = len(texts)
        for i in range(0, total_docs, batch_size):
            batch_texts = texts[i : i + batch_size]
            batch_metadatas = metadatas[i : i + batch_size]
            print(
                f"      â†³ Batch ekleniyor: {i} - {i + len(batch_texts)} / {total_docs}"
            )
            self.vectorstore.add_texts(texts=batch_texts, metadatas=batch_metadatas)

    # --- RAPTOR Core FonksiyonlarÄ± (Ã–nceki koddan) ---
    def _cluster_and_summarize(
        self, documents: List[Document], filename: str, file_hash: str
    ):
        """Sadece yeni gelen dokÃ¼manlar iÃ§in RAPTOR aÄŸacÄ± oluÅŸturur."""
        print(f"ğŸš€ RAPTOR BaÅŸlatÄ±lÄ±yor: {filename} ({len(documents)} chunk)")

        # 1. Katman 0 (Orijinaller) Ekleme
        current_texts = [doc.page_content for doc in documents]
        current_metadatas = []
        for doc in documents:
            meta = doc.metadata.copy()
            meta.update(
                {
                    "layer": 0,
                    "type": "original",
                    "original_source": filename,
                    "file_hash": file_hash,
                }
            )
            current_metadatas.append(meta)

        self._add_texts_in_batches(current_texts, current_metadatas, batch_size=5000)

        # RAPTOR DÃ¶ngÃ¼sÃ¼ (BasitleÅŸtirilmiÅŸ max_layer=3)
        max_layers = 3
        for layer in range(1, max_layers + 1):
            embeddings = np.array(EMBEDDING_MODEL.embed_documents(current_texts))

            if len(embeddings) <= 5:
                break  # Yetersiz veri

            # UMAP & GMM
            n_neighbors = min(10, len(embeddings) - 1)
            umap_reducer = umap.UMAP(
                n_neighbors=n_neighbors,
                n_components=2,
                metric="cosine",
                random_state=42,
            )
            reduced_emb = umap_reducer.fit_transform(embeddings)

            n_clusters = int(np.sqrt(len(embeddings)))
            gmm = GMM(n_components=n_clusters, random_state=42)
            gmm.fit(reduced_emb)
            labels = gmm.predict(reduced_emb)

            # Ã–zetleme DÃ¶ngÃ¼sÃ¼
            df = pd.DataFrame({"text": current_texts, "cluster": labels})
            new_texts = []
            new_metadatas = []

            print(f"   âš™ï¸  Katman {layer}: {n_clusters} adet kÃ¼me Ã¶zetleniyor...")

            unique_clusters = df["cluster"].unique()
            for cluster_id in tqdm(unique_clusters, desc=f"Katman {layer} Ä°lerlemesi"):

                cluster_docs = df[df["cluster"] == cluster_id]["text"].tolist()
                combined_text = "\n".join(cluster_docs)

                # --- GÃœVENLÄ°K Ã–NLEMÄ° ---
                # EÄŸer kÃ¼medeki metin Ã§ok Ã§ok uzunsa (Ollama 8192 token limitini aÅŸarsa)
                # takÄ±lma yapabilir. Ä°lk 25.000 karakteri alÄ±p keselim.
                if len(combined_text) > 25000:
                    combined_text = combined_text[:25000]

                # LLM Ã–zetleme
                prompt = ChatPromptTemplate.from_template(
                    "Metinleri TÃ¼rkÃ§e Ã¶zetle. Sadece Ã¶zeti yaz: {context}"
                )
                chain = prompt | LLM_MODEL | StrOutputParser()
                summary = chain.invoke({"context": combined_text})

                new_texts.append(summary)
                new_metadatas.append(
                    {
                        "layer": layer,
                        "type": "summary",
                        "original_source": filename,
                        "file_hash": file_hash,
                        "cluster_id": int(cluster_id),
                    }
                )

            if new_texts:
                # Ã–zetlerde genelde sayÄ± azdÄ±r ama garanti olsun diye burayÄ± da deÄŸiÅŸtirelim
                self._add_texts_in_batches(new_texts, new_metadatas, batch_size=5000)
                current_texts = new_texts
            else:
                break

    def sync_folder(self):
        """KlasÃ¶rÃ¼ tarar ve gerekli gÃ¼ncellemeleri yapar."""
        json_files = glob.glob(os.path.join(DB_FOLDER, "*.json"))

        if not json_files:
            print("âš ï¸ KlasÃ¶rde .json dosyasÄ± bulunamadÄ±.")
            return

        print(f"ğŸ“‚ Bulunan dosyalar: {[os.path.basename(f) for f in json_files]}")

        for file_path in json_files:
            filename = os.path.basename(file_path)
            current_hash = self._calculate_file_hash(file_path)

            # KONTROL: GÃ¼ncel mi?
            is_synced = self._check_if_exists_and_current(filename, current_hash)

            if is_synced:
                print(f"âœ… [ATLANDI] {filename} zaten gÃ¼ncel.")
            else:
                print(f"ğŸ”„ [GÃœNCELLENÄ°YOR] {filename} deÄŸiÅŸmiÅŸ veya yeni.")

                # 1. Eski veriyi temizle (EÄŸer varsa)
                self._delete_file_from_db(filename)

                # 2. DosyayÄ± oku ve parÃ§ala
                docs = self._load_json_and_split(file_path)

                if docs:
                    # 3. RAPTOR iÅŸlemini baÅŸlat
                    self._cluster_and_summarize(docs, filename, current_hash)
                    print(f"ğŸ‰ {filename} baÅŸarÄ±yla iÅŸlendi.")


def rebuild_db(collection_name):
    client = chromadb.HttpClient(host=CHROMA_HOST, port=CHROMA_PORT)
    try:
        # Koleksiyonu sil
        client.delete_collection(name=collection_name)
        print(f"âœ… '{collection_name}' koleksiyonu tamamen silindi.")
    except Exception as e:
        print(f"Hata veya koleksiyon zaten yok: {e}")
    if not os.path.exists(DB_FOLDER):
        os.makedirs(DB_FOLDER)
        print(f"LÃ¼tfen '{DB_FOLDER}' klasÃ¶rÃ¼ne json dosyalarÄ±nÄ±zÄ± koyun.")
    else:
        manager = RaptorManager()
        manager.sync_folder()


# --- Ã‡ALIÅTIRMA ---
if __name__ == "__main__":
    if not os.path.exists(DB_FOLDER):
        os.makedirs(DB_FOLDER)
        print(f"LÃ¼tfen '{DB_FOLDER}' klasÃ¶rÃ¼ne json dosyalarÄ±nÄ±zÄ± koyun.")
    else:
        rebuild_db(collection_name=COLLECTION_NAME)
